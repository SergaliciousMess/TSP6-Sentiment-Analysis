'''
    @author SergaliciousMess
    @author emyasenc
    
    Description: an edited verison of primary Python file for the sentiment analysis class, originally commited from @author SergaliciousMess
                 Note: The following are further tasks to take into consideration for the CNN model for this class:
                 
                Neutral Sentiment: Define a clear criterion for neutral sentiment. It's a subjective decision and may require additional analysis of the dataset (when we acquire the labeled data).
                Evaluation Metrics: Consider implementing evaluation metrics such as accuracy, precision, recall, and F1-score to assess model performance during training and testing.
                Hyperparameter Tuning: Experiment with different hyperparameters such as learning rate, batch size, and model architecture to improve model performance.
                Error Handling: Implement error handling mechanisms to gracefully handle exceptions and errors during data processing and model training.
                
                
Downlaod spacy to your device before running:                
To use spacy in Python code, install the library using pip:
pip install spacy

Also need to download language models for the languages one intends to process. For example, download the English language model using the following command:
python -m spacy download en_core_web_sm
   '''

import csv
import json
import torch
import argparse
import spacy
import numpy as np
import torch.nn.functional as F
from torch import nn
from torch.utils.data import DataLoader
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from torchtext.data.functional import to_map_style_dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

#device- desired device for computations
#data_type- desired number format
#optimizer- lambda function for desired pytorch optimization algorithm
#learning_rate- desired learning rate for optimization
#loss_function- lambda function for desired pytorch loss calculation algorithm

def ground_truth_label_for_text(text):
    # Assuming each text sample is a tuple where the label is the first element
    return text[0]

class CNNModel(nn.Module):
    def __init__(self, architecture='standard', input_size=64, output_size=1):
        super(CNNModel, self).__init__()
        self.architecture = architecture
        # Define the CNN architecture based on the chosen type
        if architecture == 'standard':
            self.model = nn.Sequential(
                nn.Linear(input_size, output_size),
                nn.Sigmoid()
            )
        elif architecture == '1d':
            self.model = nn.Sequential(
                nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3),
                nn.MaxPool1d(kernel_size=2),
                nn.Flatten(),
                nn.Linear(64 * ((input_size - 2) // 2), output_size),
                nn.Sigmoid()
            )
        elif architecture == 'wide':
            self.model = nn.Sequential(
                nn.Conv1d(in_channels=input_size, out_channels=128, kernel_size=5),  # Increase out_channels for wider architecture
                nn.MaxPool1d(kernel_size=2),
                nn.Flatten(),
                nn.Linear(128 * ((input_size - 4) // 2), output_size),  # Adjust input_size calculation for wider architecture
                nn.Sigmoid()
            )
        elif architecture == 'deep':
            self.model = nn.Sequential(
                nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3),
                nn.ReLU(),
                nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3),
                nn.ReLU(),
                nn.MaxPool1d(kernel_size=2),
                nn.Flatten(),
                nn.Linear(64 * ((input_size - 2) // 2), output_size),
                nn.Sigmoid()
            )
        elif architecture == 'dilated':
            self.model = nn.Sequential(
                nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, dilation=2),
                nn.MaxPool1d(kernel_size=2),
                nn.Flatten(),
                nn.Linear(64 * ((input_size - 4) // 2), output_size),
                nn.Sigmoid()
            )
        elif architecture == 'parallel':
            self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=32, kernel_size=3)
            self.conv2 = nn.Conv1d(in_channels=input_size, out_channels=32, kernel_size=5)
            self.conv3 = nn.Conv1d(in_channels=input_size, out_channels=32, kernel_size=7)
            self.pool = nn.MaxPool1d(kernel_size=2)
            self.fc = nn.Linear(32 * ((input_size - 6) // 2), output_size)

    def forward(self, x):
        if hasattr(self, 'model'):
            return self.model(x)
        elif hasattr(self, 'conv1'):
            # Apply convolutional operations followed by ReLU activation
            x1 = F.relu(self.conv1(x))  # Apply convolution and ReLU activation to input x
            x2 = F.relu(self.conv2(x))  # Apply convolution and ReLU activation to input x
            x3 = F.relu(self.conv3(x))  # Apply convolution and ReLU activation to input x
        
            # Apply max pooling to reduce spatial dimensions
            x1 = self.pool(x1)  # Apply max pooling to the conv1 result
            x2 = self.pool(x2)  # Apply max pooling to the conv2 result
            x3 = self.pool(x3)  # Apply max pooling to the conv3 result
        
            # Concatenate the pooled feature maps along the channel dimension
            x = torch.cat((x1, x2, x3), dim=1)  # Concatenate the pooled feature maps along the channel dimension
        
            # Flatten the concatenated feature maps
            x = x.view(x.size(0), -1)  # Flatten the concatenated feature maps
        
            # Apply fully connected layer
            x = self.fc(x)  # Apply fully connected layer to the flattened feature maps
        
            # Apply sigmoid activation function to the output
            return torch.sigmoid(x)  # Apply sigmoid activation function to the output
        
        return self.model(x)

class TextClassifier(nn.Module):
    def __init__(self, bag, network):
        super(TextClassifier, self).__init__()
        self.bag = bag
        self.network = network

    def forward(self, input):
        return self.network(self.bag(input))

class SentimentAnalysis():
    def __init__(self, dataset, model_type='standard', embedding_width=64, num_classes=1, learning_rate=0.001, optimizer=torch.optim.Adam, loss_function=nn.CrossEntropyLoss(), data_type = torch.float32):
        #Initialize nlp and build vocab
        torch.set_default_dtype(data_type)
        self.nlp = spacy.load('en_core_web_sm')
        self.vocab = self.build_vocab(dataset)

        #Construct model based off of parameters
        bag = nn.EmbeddingBag(num_embeddings=len(self.vocab), embedding_dim=embedding_width)
        bag.to(data_type)
        network = CNNModel(architecture=model_type, input_size=embedding_width, output_size=num_classes)
        network.to(data_type)
        self.model = TextClassifier(bag = bag, network = network)
        
        #store some important variables
        self.data_type = data_type
        self.embedding_width = embedding_width

        # Initialize optimizer and loss function
        self.learning_rate = learning_rate
        self.optimizer = optimizer(self.model.parameters(), lr=learning_rate)
        self.loss_function = loss_function

    def tokenize(self, text : str):
        # Tokenization logic using spaCy
        doc = self.nlp(text)
        tokens = [token.lemma_.lower().strip() for token in doc if not token.is_stop and not token.is_punct]
        return tokens
    def build_vocab(self, dataloader):
        # Build vocabulary from the tokenized data

        #put every token in every entry into the tokens list
        tokens = []
        for batch in dataloader:
            for text in batch[1]:
                for token in self.tokenize(text): #first element in entry is label, second is text
                    tokens.append(token)
        
        vocab = build_vocab_from_iterator(tokens, specials=["<unk>"])
        vocab.set_default_index(vocab["<unk>"])
        return vocab
    
    def text_to_numerical(self, text):
        # Convert tokenized text to numerical representation using vocabulary
        numerical_representation = [self.vocab[token] for token in self.tokenize(text)]
        return numerical_representation

    def analyze(self, input_text: str):
        numerical_representation = self.text_to_numerical(input_text)
        input_array = np.array(numerical_representation)
        input_tensor = torch.tensor(input_array, dtype=torch.float32)
        output = self.model(input_tensor)
        positive_threshold = 0.7 
        negative_threshold = 0.3 
        sentiment_labels = []
        for probability in output:
            if probability >= positive_threshold:
                sentiment_labels.append("Positive")
            elif probability <= negative_threshold:
                sentiment_labels.append("Negative")
            else:
                sentiment_labels.append("Neutral")
        return sentiment_labels
    
    def train(self, train_dataloader, epochs=10):
        #FIXME: convert train_dataloader entries from tuple to tensors
        for epoch in range(epochs):
            running_loss = 0.0
            for labels, inputs in train_dataloader:
                self.optimizer.zero_grad()
                outputs = self.model(inputs)
                loss = self.loss_function(outputs, labels)
                loss.backward()
                self.optimizer.step()
                running_loss += loss.item()
            print(f"Epoch {epoch+1}, Loss: {running_loss}")

    def train2(self, dataloader, epochs=10):
        #collect training values
        labels = []
        text_tokens = []
        for batch in dataloader:
            for label in batch[0]:
                labels.append(label)
            for text in batch[1]:
                text_tokens.append(self.text_to_numerical(text))

        #convert isolated lists into tensors
        labels = torch.tensor(labels)
        labels = labels.to(self.data_type)
        
        for i in range(len(text_tokens)):
            while len(text_tokens[i]) < self.embedding_width:
                text_tokens[i].append(0)
        text_tokens = torch.tensor(text_tokens)

#FIXME: data type issues
        for epoch in range(epochs):
            self.optimizer.zero_grad() 
            predictions = self.model(text_tokens)
            predictions = torch.flatten(predictions)
            loss = self.loss_function(predictions, labels)
            loss.backward()
            self.optimizer.step()

# Function to load a dataset from a given path, which can accept csv, json, or txt formats
        #FIXME: FIX LABEL/DATA ENTRIES FOR JSON AND TXT
def load_dataset(dataset_path, format):
    if format == 'csv':
        dataset = []
        with open(dataset_path, 'r', encoding='utf-8') as file:
            csv_reader = csv.reader(file)
            for row in csv_reader:
                # Assuming each row represents a sample
                dataset.append( (int(row[0]), row[1].strip()) )  # Assuming the text is in the second column
        return dataset
    elif format == 'json':
        dataset = []
        with open(dataset_path, 'r', encoding='utf-8') as file:
            json_data = json.load(file)
            for item in json_data:
                # Assuming each item represents a sample
                dataset.append(item['text'].strip())  # Assuming the text is stored under the key 'text'
        return dataset
    elif format == 'txt':
        dataset = []
        with open(dataset_path, 'r', encoding='utf-8') as file:
            for line in file:
                dataset.append(line.strip())
        return dataset
    else:
        raise ValueError("Unsupported dataset format. Supported formats: 'csv', 'json', 'txt'")

def main():
     # Parse command-line arguments
    parser = argparse.ArgumentParser(description='Sentiment Analysis CLI')
    parser.add_argument('--architecture', choices=['standard', '1d', 'deep', 'wide', 'parallel', 'dilated', 'attention'], default='standard', help='Select the CNN architecture (default: standard)')
    parser.add_argument('--learning-rate', type=float, default=0.001, help='Learning rate for training (default: 0.001)')
    parser.add_argument('--batch-size', type=int, default=64, help='Batch size for training (default: 64)')
    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs for training (default: 10)')
    parser.add_argument('--dataset', type=str, required=True, help='Path to the dataset file')
    parser.add_argument('--positive-threshold', type=float, default=0.7, help='Threshold for classifying as Positive (default: 0.7)')
    parser.add_argument('--negative-threshold', type=float, default=0.3, help='Threshold for classifying as Negative (default: 0.3)')

    args = parser.parse_args()
    
    # Execute functionality based on command-line arguments
    print(f"Architecture: {args.architecture}")
    print(f"Learning Rate: {args.learning_rate}")
    print(f"Batch Size: {args.batch_size}")
    print(f"Epochs: {args.epochs}")
    print(f"Dataset: {args.dataset}")
    print(f"Positive Threshold: {args.positive_threshold}")
    print(f"Negative Threshold: {args.negative_threshold}")
    
    # Load dataset from the specified file path
    dataset = load_dataset(args.dataset, 'csv') #FIXME- hardcoding csv so it works for testing this data

    # Split the dataset into training and testing sets
    train_texts, test_texts = train_test_split(dataset, test_size=0.2, random_state=42)

    # Initialize the CNN model with the specified architecture and hyperparameters
    model = CNNModel(architecture=args.architecture)

    # Convert data iterators to map-style datasets
    train_dataloader = DataLoader(train_texts, batch_size=args.batch_size, shuffle=True)

    # Initialize SentimentAnalysis with the selected model
    sentiment_analysis = SentimentAnalysis(dataset=train_dataloader, model_type=args.architecture, learning_rate=args.learning_rate)

    # Train the model
    sentiment_analysis.train2(dataloader=train_dataloader, epochs=args.epochs)

    # Evaluate the model
    predictions = []
    targets = []
    for text in test_texts:
        prediction = sentiment_analysis.analyze(text)
        predictions.extend(prediction)
        # Assuming that labels are inferred from text or dataset structure
        targets.extend(ground_truth_label_for_text(text))

    # Calculate performance metrics
    accuracy = accuracy_score(targets, predictions)
    precision = precision_score(targets, predictions, average='weighted')
    recall = recall_score(targets, predictions, average='weighted')
    f1 = f1_score(targets, predictions, average='weighted')

    # Print performance metrics
    print(f"Accuracy: {accuracy}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"F1 Score: {f1}")


if __name__ == "__main__":
   main()
